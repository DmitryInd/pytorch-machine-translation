{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a62d887-01aa-4980-9220-3bf28c077504",
   "metadata": {},
   "source": [
    "### Домашнее задание Transformers Training (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d7452-febe-4d25-a9c1-005fcc26b35b",
   "metadata": {},
   "source": [
    "В этом домашнем задании требуется обучить несколько Transformer-based моделей в задаче машинного перевода. Для обучения можно воспользоваться текущим проектом, так и реализовать свой пайплайн обучения. Если будете использовать проект, теги **TODO** проекта отмечают, какие компоненты надо реализовать.\n",
    "В ноутбуке нужно только отобразить результаты обучения и выводы. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Ваш код обучения нужно выложить на ваш github, в строке ниже дать ссылку на него. В первую очередь будут оцениваться результаты в ноутбуке, код нужен для проверки адекватности результатов. \n",
    "\n",
    "Обучать модели до конца не нужно, только для демонстрации, что модель обучается и рабочая - снижение val_loss, рост bleu_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691e7b9-6538-42b1-96b8-838f2efab4af",
   "metadata": {},
   "source": [
    "#### Сcылка на ваш github с проектом(вставить свой) - https://github.com/runnerup96/pytorch-machine-translation\n",
    "\n",
    "Ноутбук с результатами выкладывать на ваш **google диск** курса. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca1e62-b210-4426-9854-55b652417a4b",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "`\n",
    "wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip\n",
    "`\n",
    "\n",
    "Модели нужно обучить на задаче перевода с английского на русский."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-08 16:25:39--  https://www.manythings.org/anki/rus-eng.zip\r\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\r\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 15460248 (15M) [application/zip]\r\n",
      "Saving to: ‘rus-eng.zip’\r\n",
      "\r\n",
      "rus-eng.zip         100%[===================>]  14.74M   347KB/s    in 51s     \r\n",
      "\r\n",
      "2023-05-08 16:26:31 (297 KB/s) - ‘rus-eng.zip’ saved [15460248/15460248]\r\n",
      "\r\n",
      "Archive:  rus-eng.zip\r\n",
      "  inflating: rus.txt                 \r\n",
      "  inflating: _about.txt              \r\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T13:26:31.912167149Z",
     "start_time": "2023-05-08T13:25:39.751928002Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "cc772a6e-7d1a-4d8d-8024-0454a948835b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Обучение Seq2seq Transformer модель(25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Transformer. В качестве блока трансформера можно использовать https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html. В качестве токенизатора воспользуйтесь HuggingFace токенизатор для source/target языков - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. \n",
    "\n",
    "Не забудьте остальные элементы модели:\n",
    "* Мы можем использовать 1 трансформер как энкодер - декодером будет выступать линейный слой. \n",
    "* Обучите свой BPE токенизатор - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "* Матрицу эмбеддингов токенов\n",
    "* Матрицу позиционных эмбеддингов\n",
    "* Линейный слой проекции в target словарь\n",
    "* Функцию маскирования будущих состояний attention, так как модель авто-регрессионна\n",
    "* Learning rate scheduler\n",
    "\n",
    "\n",
    "В качестве результатов, приложите следующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, размерность скрытого слоя, количетсво слоев\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "length = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T16:55:47.036264123Z",
     "start_time": "2023-05-08T16:55:47.034724911Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a7ce070-1e45-47ac-a07a-c36b6baa114e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T16:56:24.124149934Z",
     "start_time": "2023-05-08T16:56:24.123349498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.one_hot(torch.full((10, ), 2), length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245297ae-d62b-4227-9312-ccff5a1c13c9",
   "metadata": {},
   "source": [
    "### Fine-tune pretrained T5 (25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Pretrained T5. Воспользуйтесь https://huggingface.co/docs/transformers/model_doc/t5 предобученной моделью. В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Не забудьте важные аспекты обучения модели:\n",
    "* Взять готовый t5 токенизатор\n",
    "* Resize matrix embedding - скорей всего ваша матрица эмбеддингов не будет включать эмбеддинги из вашего сета. Пример обновления матрицы эмбеддингов тут тут https://github.com/runnerup96/Transformers-Tuning/blob/main/t5_encoder_decoder.py\n",
    "* Learning rate scheduler/Adafactor with constant learning rate\n",
    "\n",
    "\n",
    "В качестве результатов, приложите следующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, pretrained model name\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5993bdc9-44f6-4af8-8e46-36d4e6824ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
