{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a62d887-01aa-4980-9220-3bf28c077504",
   "metadata": {},
   "source": [
    "### Домашнее задание Transformers Training (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d7452-febe-4d25-a9c1-005fcc26b35b",
   "metadata": {},
   "source": [
    "В этом домашнем задании требуется обучить несколько Transformer-based моделей в задаче машинного перевода. Для обучения можно воспользоваться текущим проектом, так и реализовать свой пайплайн обучения. Если будете использовать проект, теги **TODO** проекта отмечают, какие компоненты надо реализовать.\n",
    "В ноутбуке нужно только отобразить результаты обучения и выводы. Архитектура модели(количество слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Ваш код обучения нужно выложить на ваш github, в строке ниже дать ссылку на него. В первую очередь будут оцениваться результаты в ноутбуке, код нужен для проверки адекватности результатов. \n",
    "\n",
    "Обучать модели до конца не нужно, только для демонстрации, что модель обучается и рабочая - снижение val_loss, рост bleu_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691e7b9-6538-42b1-96b8-838f2efab4af",
   "metadata": {},
   "source": [
    "#### Сcылка на ваш github с проектом - https://github.com/DmitryInd/pytorch-machine-translation\n",
    "\n",
    "Ноутбук с результатами выкладывать на ваш **google диск** курса. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca1e62-b210-4426-9854-55b652417a4b",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "`\n",
    "wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip\n",
    "`\n",
    "\n",
    "Модели нужно обучить на задаче перевода с английского на русский."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c0cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:07:55.297768491Z",
     "start_time": "2023-05-12T19:07:55.296872444Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099746a5",
   "metadata": {},
   "source": [
    "### Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132c39f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:07:55.302043889Z",
     "start_time": "2023-05-12T19:07:55.298218824Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Кажется, у меня проблемы с драйверами для видеокарты, из-за чего обучение может в произвольный момент падать\n",
    "# Запрет асинхронных запусков ядра не решает проблему полностью, но заметно снижает вероятность её возникновения\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd9a60-bb1c-4ac9-a035-40be71c20e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, \"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341eecf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:07:56.006818409Z",
     "start_time": "2023-05-12T19:07:55.302454224Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad7ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:07:56.009481420Z",
     "start_time": "2023-05-12T19:07:56.008293261Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_learning_parameters(data_config, model_config):\n",
    "    print(f\"Parameters of the model:\")\n",
    "    print(f\"* Learning rate is {model_config['learning_rate']}.\")\n",
    "    if 'div_factor' in model_config:\n",
    "        print(f\"* OneCycleLR with warm up and linear decrease of learning rate to division factor {model_config['div_factor']} is in use.\")\n",
    "    else:\n",
    "        print(\"* Adafactor with constant learning rate is in use.\")\n",
    "    print(f\"* Batch size is {data_config['batch_size']}.\")\n",
    "    if data_config['pretrained_input_tokenizer_name'] is not None:\n",
    "        print(f\"* The tokenizer for input values uses the pretrained vocabulary from '{data_config['pretrained_input_tokenizer_name']}'.\")\n",
    "    if data_config['pretrained_output_tokenizer_name'] is not None:\n",
    "        print(f\"* The tokenizer for output values uses the pretrained vocabulary from '{data_config['pretrained_output_tokenizer_name']}'.\")\n",
    "    print(f\"* The number of epochs is {model_config['epoch_num']}.\")\n",
    "    if \"pretrained_model_name\" in model_config:\n",
    "        print(f\"* Pretrained parameters from '{model_config['pretrained_model_name']}' is used.\")\n",
    "    if 'emb_size' in model_config:\n",
    "        print(f\"* The size of embeddings/hidden states is {model_config['emb_size']}.\")\n",
    "    if 'num_encoder_layers' in model_config and 'num_decoder_layers' in model_config:\n",
    "        print(f\"* The numbers of encoder and decoder layers are {model_config['num_encoder_layers']} and {model_config['num_decoder_layers']}.\")\n",
    "    if 'num_heads' in model_config:\n",
    "        print(f\"* There are {model_config['num_heads']} heads for self-attention and encoder-decoder attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd770e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:07:56.051737317Z",
     "start_time": "2023-05-12T19:07:56.010997870Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MATCH_REGEX = re.compile(r\"[+-]?\\b(\\d+([.]\\d*)?([eE][+-]?\\d+)?|[.]\\d+([eE][+-]?\\d+)?)\\b\")\n",
    "\n",
    "def extract_values(string):\n",
    "    result = dict()\n",
    "    params = ['val_loss', 'train_loss', 'bleu_score']\n",
    "    if \"train_loss\" in string:\n",
    "        found_vals = [re_match[0] for re_match in re.findall(MATCH_REGEX, string)]\n",
    "\n",
    "        if len(params) == len(found_vals):\n",
    "            for name, val in zip(params, found_vals):\n",
    "                result[name] = float(val)\n",
    "    return result\n",
    "\n",
    "def plot_results(train_loss_list, val_loss_list, val_bleu_list, run_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "    ax1.plot(range(len(train_loss_list)), train_loss_list, label='train loss')\n",
    "    ax1.plot(range(len(val_loss_list)), val_loss_list, label='val loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(range(len(val_bleu_list)), val_bleu_list, label='val bleu')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('BLEU')\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.suptitle(run_name, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3417ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:07:56.088079675Z",
     "start_time": "2023-05-12T19:07:56.052112915Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_demo_translations(model, val_dataloader, input_tokenizer, sentence_num=10):\n",
    "    input_tensor, target_tensor = val_dataloader.__next__()\n",
    "    input_tensor = input_tensor[:sentence_num]\n",
    "    target_tensor = target_tensor[:sentence_num]\n",
    "    with torch.no_grad():\n",
    "        predicted_samples, _ = model.forward(input_tensor)\n",
    "    bleu_score, actual_sentences, predicted_sentences = model.eval_bleu(predicted_samples, target_tensor)\n",
    "    print(f'BLEU score on the following sentences is {bleu_score}.')\n",
    "    print(\"Original sentence | Predicted translation | True translation\")\n",
    "    for in_tens, pred, actual in zip(input_tensor, actual_sentences, predicted_sentences):\n",
    "        print(f\"{input_tokenizer(in_tens)} | {pred} | {actual}\")\n",
    "    print('##############################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc772a6e-7d1a-4d8d-8024-0454a948835b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Обучение Seq2seq Transformer модель(25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Transformer. В качестве блока трансформера можно использовать https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html. В качестве токенизатора воспользуйтесь HuggingFace токенизатор для source/target языков - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. \n",
    "\n",
    "Не забудьте остальные элементы модели:\n",
    "* Мы можем использовать 1 трансформер как энкодер - декодером будет выступать линейный слой. \n",
    "* Обучите свой BPE токенизатор - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "* Матрицу эмбеддингов токенов\n",
    "* Матрицу позиционных эмбеддингов\n",
    "* Линейный слой проекции в target словарь\n",
    "* Функцию маскирования будущих состояний attention, так как модель авто-регрессионна\n",
    "* Learning rate scheduler\n",
    "\n",
    "\n",
    "В качестве результатов, приложите следующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, размерность скрытого слоя, количество слоев\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d8dad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:07:56.264799322Z",
     "start_time": "2023-05-12T19:07:56.076891617Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from models import trainer\n",
    "from data.datamodule import DataManager\n",
    "from txt_logger import TXTLogger\n",
    "from models.seq2seq_transformer import Seq2SeqTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674c56b",
   "metadata": {},
   "source": [
    "### Конфигурация модели и алгоритма обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402335f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:07:56.305732206Z",
     "start_time": "2023-05-12T19:07:56.305448961Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "transformer_data_config = yaml.load(open(\"configs/data_config.yaml\", 'r'),   Loader=yaml.Loader)\n",
    "transformer_model_config = yaml.load(open(\"configs/transformer_config.yaml\", 'r'),   Loader=yaml.Loader)\n",
    "print_learning_parameters(transformer_data_config, transformer_model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23e360",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143ab44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:08:35.074195107Z",
     "start_time": "2023-05-12T19:07:56.305860781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tran_dm = DataManager(transformer_data_config, DEVICE)\n",
    "tran_train_dataloader, tran_val_dataloader = tran_dm.prepare_data()\n",
    "\n",
    "transformer = Seq2SeqTransformer(\n",
    "    device=DEVICE,\n",
    "    encoder_vocab_size=len(tran_dm.source_tokenizer.index2word),\n",
    "    decoder_vocab_size=len(tran_dm.target_tokenizer.index2word),\n",
    "    target_tokenizer=tran_dm.target_tokenizer,\n",
    "    start_symbol=tran_dm.target_tokenizer.sos_token,\n",
    "    lr=transformer_model_config['learning_rate'],\n",
    "    total_steps=transformer_model_config['epoch_num']*len(tran_train_dataloader),\n",
    "    emb_size=transformer_model_config['emb_size'],\n",
    "    num_heads=transformer_model_config['num_heads'],\n",
    "    num_encoder_layers=transformer_model_config['num_encoder_layers'],\n",
    "    num_decoder_layers=transformer_model_config['num_decoder_layers'],\n",
    "    dropout=transformer_model_config['dropout'],\n",
    "    div_factor=transformer_model_config['div_factor']\n",
    ")\n",
    "\n",
    "tran_logger = TXTLogger(transformer_model_config['path_to_log'])\n",
    "tran_trainer_cls = trainer.Trainer(model=transformer, model_config=transformer_model_config, logger=tran_logger)\n",
    "\n",
    "if transformer_model_config['try_one_batch']:\n",
    "    tran_train_dataloader = [list(tran_train_dataloader)[0]]\n",
    "    tran_val_dataloader = [list(tran_val_dataloader)[0]]\n",
    "\n",
    "tran_trainer_cls.train(tran_train_dataloader, tran_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac41f18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "file_content = open(os.path.join(transformer_model_config['path_to_log'], \"progress_log.txt\"), 'r').read().split(\"\\n\")\n",
    "tran_train_loss_list, tran_val_loss_list, tran_val_bleu_list = [], [], []\n",
    "for line in file_content:\n",
    "    d = extract_values(line)\n",
    "    if len(d) > 0:\n",
    "        tran_train_loss_list.append(d['train_loss'])\n",
    "        tran_val_loss_list.append(d['val_loss'])\n",
    "        tran_val_bleu_list.append(d['bleu_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab2ecb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_results(tran_train_loss_list, tran_val_loss_list, tran_val_bleu_list, run_name='Transformer training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e798051",
   "metadata": {},
   "source": [
    "### Итоговое качество модели/примеры переводов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd3baa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Final BLUE score is {tran_val_bleu_list[-1]}.\")\n",
    "print(f\"The best BLUE score is {max(tran_val_bleu_list)} on {np.argmax(tran_val_bleu_list) + 1} step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a5832",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print_demo_translations(transformer, tran_val_dataloader, tran_dm.source_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245297ae-d62b-4227-9312-ccff5a1c13c9",
   "metadata": {},
   "source": [
    "### Fine-tune pretrained T5 (25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Pretrained T5. Воспользуйтесь https://huggingface.co/docs/transformers/model_doc/t5 предобученной моделью. В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. Архитектура модели(количество слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Не забудьте важные аспекты обучения модели:\n",
    "* Взять готовый t5 токенизатор\n",
    "* Resize matrix embedding - скорей всего ваша матрица эмбеддингов не будет включать эмбеддинги из вашего сета. Пример обновления матрицы эмбеддингов тут тут https://github.com/runnerup96/Transformers-Tuning/blob/main/t5_encoder_decoder.py\n",
    "* Learning rate scheduler/Adafactor with constant learning rate\n",
    "\n",
    "\n",
    "В качестве результатов, приложите следующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, pretrained model name\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80425415-d6e3-4b12-a132-0eb3d3127eea",
   "metadata": {},
   "source": [
    "### Конфигурация модели и алгоритма обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7444068-4c36-4349-81cd-79752ab2f27e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:07:56.305732206Z",
     "start_time": "2023-05-12T19:07:56.305448961Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "t5_data_config = yaml.load(open(\"configs/data_config.yaml\", 'r'),   Loader=yaml.Loader)\n",
    "t5_model_config = yaml.load(open(\"configs/transformer_config.yaml\", 'r'),   Loader=yaml.Loader)\n",
    "print_learning_parameters(t5_data_config, t5_model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919fd5f-170f-44b1-9f78-b1872b1a0994",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12120ce4-c495-44d0-8116-19b5d75eb19b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T19:08:35.074195107Z",
     "start_time": "2023-05-12T19:07:56.305860781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t5_dm = DataManager(t5_data_config, DEVICE)\n",
    "t5_train_dataloader, t5_val_dataloader = t5_dm.prepare_data()\n",
    "\n",
    "t5_model = Seq2SeqTransformer(\n",
    "    device=DEVICE,\n",
    "    encoder_vocab_size=len(tran_dm.source_tokenizer.index2word),\n",
    "    decoder_vocab_size=len(tran_dm.target_tokenizer.index2word),\n",
    "    target_tokenizer=tran_dm.target_tokenizer,\n",
    "    start_symbol=tran_dm.target_tokenizer.sos_token,\n",
    "    lr=transformer_model_config['learning_rate'],\n",
    "    total_steps=transformer_model_config['epoch_num']*len(tran_train_dataloader),\n",
    "    emb_size=transformer_model_config['emb_size'],\n",
    "    num_heads=transformer_model_config['num_heads'],\n",
    "    num_encoder_layers=transformer_model_config['num_encoder_layers'],\n",
    "    num_decoder_layers=transformer_model_config['num_decoder_layers'],\n",
    "    dropout=transformer_model_config['dropout'],\n",
    "    div_factor=transformer_model_config['div_factor']\n",
    ")\n",
    "\n",
    "t5_logger = TXTLogger(t5_model_config['path_to_log'])\n",
    "t5_trainer_cls = trainer.Trainer(model=t5_model, model_config=t5_model_config, logger=t5_logger)\n",
    "\n",
    "if t5_model_config['try_one_batch']:\n",
    "    train_dataloader = [list(t5_train_dataloader)[0]]\n",
    "    tran_val_dataloader = [list(t5_val_dataloader)[0]]\n",
    "\n",
    "t5_trainer_cls.train(t5_train_dataloader, t5_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7849d-f100-4f56-9d4e-b532334d989a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "file_content = open(os.path.join(transformer_model_config['path_to_log'], \"progress_log.txt\"), 'r').read().split(\"\\n\")\n",
    "t5_train_loss_list, t5_val_loss_list, t5_val_bleu_list = [], [], []\n",
    "for line in file_content:\n",
    "    d = extract_values(line)\n",
    "    if len(d) > 0:\n",
    "        t5_train_loss_list.append(d['train_loss'])\n",
    "        t5_val_loss_list.append(d['val_loss'])\n",
    "        t5_val_bleu_list.append(d['bleu_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f55dc1-bcc0-463c-a151-93dc0224bb87",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_results(t5_train_loss_list, t5_val_loss_list, t5_val_bleu_list, run_name='Transformer training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61212aeb-65af-4f5f-8382-3bc77958af9c",
   "metadata": {},
   "source": [
    "### Итоговое качество модели/примеры переводов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95390274-095d-4532-9d62-f18960c6518a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Final BLUE score is {t5_val_bleu_list[-1]}.\")\n",
    "print(f\"The best BLUE score is {max(t5_val_bleu_list)} on {np.argmax(tran_val_bleu_list) + 1} step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c384dc-e578-402a-8739-9ae58d793bdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print_demo_translations(t5_model, t5_val_dataloader, t5_dm.source_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993bdc9-44f6-4af8-8e46-36d4e6824ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
